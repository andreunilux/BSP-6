{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d122036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library used for fine tuning\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "# Pandas Dataframe Library\n",
    "import json\n",
    "import pandas as pd\n",
    "# HateBert Libarary\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('Hate-speech-CNERG/dehatebert-mono-english')\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # Create a pandas DataFrame from the parsed json data\n",
    "    train_df = pd.read_json('train.jsonl', lines=True)\n",
    "\n",
    "    # Create a pandas DataFrame from the parsed json data\n",
    "    val_df = pd.read_json('val.jsonl', lines=True)\n",
    "\n",
    "    # Create a pandas DataFrame from the parsed json data\n",
    "    test_df = pd.read_json('test.jsonl', lines=True)\n",
    "\n",
    "    # Visualize each dataframe/dataset\n",
    "    print(\"TRAIN\")\n",
    "    print(train_df.head(1))\n",
    "    print(\"\\n\")\n",
    "    print(\"VAL\")\n",
    "    print(val_df.head(1))\n",
    "    print(\"\\n\")\n",
    "    print(\"TEST\")\n",
    "    print(test_df.head(1))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Tokenize text data\n",
    "    train_df['context'] = train_df.apply(lambda row: tokenizer.tokenize(row['context']), axis=1)\n",
    "    train_df['target'] = train_df.apply(lambda row: tokenizer.tokenize(row['target']), axis=1)\n",
    "    print(train_df.head())\n",
    "    \n",
    "    # Convert tokens to IDs\n",
    "    train_df['context'] = train_df.apply(lambda row: tokenizer.convert_tokens_to_ids(row['context']), axis=1)\n",
    "    train_df['target'] = train_df.apply(lambda row: tokenizer.convert_tokens_to_ids(row['target']), axis=1)\n",
    "    print(train_df.head())\n",
    "    \n",
    "    # Pad sequences to fixed length\n",
    "    max_length = 128\n",
    "    train_df['pad_context'] = train_df.apply(lambda row: row['context'] + [0] * (max_length - len(row['context'])), axis=1)\n",
    "    train_df['pad_target'] = train_df.apply(lambda row:  row['target'] + [0] * (max_length - len(row['target'])), axis=1)\n",
    "    print(train_df.head())\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d75a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "   idx  label            context                             target\n",
      "0    0      2  The UK is fucked.  >The ~~UK~~ world is fucked  FTFY\n",
      "\n",
      "\n",
      "VAL\n",
      "   idx  label                                            context  \\\n",
      "0    0      2  The fact that you think that is sufficient for...   \n",
      "\n",
      "                                              target  \n",
      "0  Not being able to find a job for 20 years soun...  \n",
      "\n",
      "\n",
      "TEST\n",
      "   idx  label                                            context  \\\n",
      "0    0      2  Someone on Tumblr actually complied a list and...   \n",
      "\n",
      "                      target  \n",
      "0  Can I get a link to that?  \n",
      "\n",
      "\n",
      "   idx  label                                            context  \\\n",
      "0    0      2                  [[UNK], [UNK], is, fuck, ##ed, .]   \n",
      "1    1      0                       [[UNK], to, this, wisdom, .]   \n",
      "2    2      1                  [\", [UNK], ', s, different, ., \"]   \n",
      "3    3      2  [[UNK], fuck, off, ##ff, ##ff, ,, this, is, ju...   \n",
      "4    4      2  [[UNK], whole, sub, should, come, to, terms, w...   \n",
      "\n",
      "                                              target  \n",
      "0  [>, [UNK], ~, ~, [UNK], ~, ~, world, is, fuck,...  \n",
      "1  [[UNK], the, [UNK], did, you, get, that, up, a...  \n",
      "2  [[UNK], ', re, right, ., [UNK], ultra, ##sou, ...  \n",
      "3  [[UNK], you, can, always, spot, a, valid, argu...  \n",
      "4  [[UNK], stopped, reading, your, history, after...  \n",
      "   idx  label                                            context  \\\n",
      "0    0      2               [100, 100, 10127, 69338, 10390, 119]   \n",
      "1    1      0                    [100, 10114, 10372, 55828, 119]   \n",
      "2    2      1              [107, 100, 112, 161, 12850, 119, 107]   \n",
      "3    3      2  [100, 69338, 11856, 13513, 13513, 117, 10372, ...   \n",
      "4    4      2  [100, 19614, 13241, 14693, 10695, 10114, 18545...   \n",
      "\n",
      "                                              target  \n",
      "0  [135, 100, 172, 172, 100, 172, 172, 10228, 101...  \n",
      "1  [100, 10103, 100, 12266, 10855, 13168, 10203, ...  \n",
      "2  [100, 112, 11449, 12873, 119, 100, 29799, 4734...  \n",
      "3  [100, 10855, 10743, 17503, 24311, 143, 51761, ...  \n",
      "4  [100, 39018, 10504, 12787, 10441, 10515, 10855...  \n",
      "   idx  label                                            context  \\\n",
      "0    0      2               [100, 100, 10127, 69338, 10390, 119]   \n",
      "1    1      0                    [100, 10114, 10372, 55828, 119]   \n",
      "2    2      1              [107, 100, 112, 161, 12850, 119, 107]   \n",
      "3    3      2  [100, 69338, 11856, 13513, 13513, 117, 10372, ...   \n",
      "4    4      2  [100, 19614, 13241, 14693, 10695, 10114, 18545...   \n",
      "\n",
      "                                              target  \\\n",
      "0  [135, 100, 172, 172, 100, 172, 172, 10228, 101...   \n",
      "1  [100, 10103, 100, 12266, 10855, 13168, 10203, ...   \n",
      "2  [100, 112, 11449, 12873, 119, 100, 29799, 4734...   \n",
      "3  [100, 10855, 10743, 17503, 24311, 143, 51761, ...   \n",
      "4  [100, 39018, 10504, 12787, 10441, 10515, 10855...   \n",
      "\n",
      "                                         pad_context  \\\n",
      "0  [100, 100, 10127, 69338, 10390, 119, 0, 0, 0, ...   \n",
      "1  [100, 10114, 10372, 55828, 119, 0, 0, 0, 0, 0,...   \n",
      "2  [107, 100, 112, 161, 12850, 119, 107, 0, 0, 0,...   \n",
      "3  [100, 69338, 11856, 13513, 13513, 117, 10372, ...   \n",
      "4  [100, 19614, 13241, 14693, 10695, 10114, 18545...   \n",
      "\n",
      "                                          pad_target  \n",
      "0  [135, 100, 172, 172, 100, 172, 172, 10228, 101...  \n",
      "1  [100, 10103, 100, 12266, 10855, 13168, 10203, ...  \n",
      "2  [100, 112, 11449, 12873, 119, 100, 29799, 4734...  \n",
      "3  [100, 10855, 10743, 17503, 24311, 143, 51761, ...  \n",
      "4  [100, 39018, 10504, 12787, 10441, 10515, 10855...  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21640/1008584645.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21640/2540529283.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mpadded_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'context'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mpadded_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a928ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
