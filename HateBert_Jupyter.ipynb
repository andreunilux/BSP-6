{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d122036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\envs\\BSP6\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Library used for fine tuning\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "# Pandas Dataframe Library\n",
    "import json\n",
    "import pandas as pd\n",
    "# HateBert Libarary\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # Open train jsonl file\n",
    "    with open('train.jsonl', 'r') as f:\n",
    "        train_data = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "    # Create a pandas DataFrame from the parsed json data\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "\n",
    "    # Open validation jsonl file\n",
    "    with open('val.jsonl', 'r') as f:\n",
    "        val_data = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "    # Create a pandas DataFrame from the parsed json data\n",
    "    val_df = pd.DataFrame(val_data)\n",
    "\n",
    "    # Open test jsonl file\n",
    "    with open('test.jsonl', 'r') as f:\n",
    "        test_data = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "    # Create a pandas DataFrame from the parsed json data\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245c8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at GroNLP/hateBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(data):\n",
    "    tokenized_data = tokenizer(\n",
    "        data[\"context\"],\n",
    "        data[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=20,\n",
    "        truncation=True\n",
    "    )\n",
    "    tokenized_data[\"label\"] = int(data[\"label\"])\n",
    "    return tokenized_data\n",
    "\n",
    "def list_of_dicts_to_dict_of_lists(d):\n",
    "    dic = d[0]\n",
    "    keys = dic.keys()\n",
    "    values = [dic.values() for dic in d]\n",
    "    return {k: list(v) for k, v in zip(keys, zip(*values))}\n",
    "\n",
    "def load_tokenized_data():\n",
    "    train_data, val_data, test_data = load_data()\n",
    "    tokenized_train = list_of_dicts_to_dict_of_lists([tokenize_data(data) for data in train_data])\n",
    "    tokenized_val = list_of_dicts_to_dict_of_lists([tokenize_data(data) for data in val_data])\n",
    "    tokenized_test = list_of_dicts_to_dict_of_lists([tokenize_data(data) for data in test_data])\n",
    "    \n",
    "    \n",
    "    return tokenized_train, tokenized_val, tokenized_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenized_train_data, tokenized_val_data, tokenized_test_data = load_tokenized_data()\n",
    "\n",
    "# load pre-trained HateBert\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"GroNLP/hateBERT\" ,num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0628145f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\envs\\BSP6\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "  Number of trainable parameters = 109486085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  1996,  2866,  2003, 21746,  1012,   102,  1028,  1996,  1066,\n",
      "         1066,  2866,  1066,  1066,  2088,  2003, 21746,  3027, 12031,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  1000,  2008,  1005,  1055,  2367,  1012,  1000,   102,  2017,\n",
      "         1005,  2128,  2157,  1012,  1996, 27312,  2758,  2009,  1005,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 4952, 2000, 2023, 9866, 1012,  102, 2073, 1996, 6616, 2106, 2017,\n",
      "        2131, 2008, 2039, 8612, 1029,  102,    0,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]), 'label': tensor(0)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 2821, 6616, 2125, 4246, 4246, 1010, 2023, 2003, 2074,  102, 8840,\n",
      "        2140, 2017, 2064, 2467, 3962, 1037, 9398,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 01:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.413535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.393437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.383625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 1996, 2755, 2008, 2017, 2228, 2008, 2003, 7182, 2005,  102, 2025,\n",
      "        2108, 2583, 2000, 2424, 1037, 3105, 2005,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  2138,  2009,  1005,  1055,  2025,  2995,  2017,  8239, 16374,\n",
      "          102,  2064,  1005,  1056,  5047,  1996,  3606, 17012,  7382,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 2054, 2055, 1996, 2033, 4168, 1000, 8840, 2140, 3287,  102, 3287,\n",
      "        4000, 2003, 7455, 2243, 1048, 2213, 7011,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  2023,  2003,  2025,  1037, 10469,  3277,  2021,  1037, 19640,\n",
      "          102,  3599,  2054,  2112,  1997,  2023,  2003,  1037, 19640,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  1000,  2008,  1005,  1055,  2367,  1012,  1000,   102,  2017,\n",
      "         1005,  2128,  2157,  1012,  1996, 27312,  2758,  2009,  1005,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 2821, 6616, 2125, 4246, 4246, 1010, 2023, 2003, 2074,  102, 8840,\n",
      "        2140, 2017, 2064, 2467, 3962, 1037, 9398,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 4952, 2000, 2023, 9866, 1012,  102, 2073, 1996, 6616, 2106, 2017,\n",
      "        2131, 2008, 2039, 8612, 1029,  102,    0,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]), 'label': tensor(0)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  1996,  2866,  2003, 21746,  1012,   102,  1028,  1996,  1066,\n",
      "         1066,  2866,  1066,  1066,  2088,  2003, 21746,  3027, 12031,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 1996, 2755, 2008, 2017, 2228, 2008, 2003, 7182, 2005,  102, 2025,\n",
      "        2108, 2583, 2000, 2424, 1037, 3105, 2005,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  2138,  2009,  1005,  1055,  2025,  2995,  2017,  8239, 16374,\n",
      "          102,  2064,  1005,  1056,  5047,  1996,  3606, 17012,  7382,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 2054, 2055, 1996, 2033, 4168, 1000, 8840, 2140, 3287,  102, 3287,\n",
      "        4000, 2003, 7455, 2243, 1048, 2213, 7011,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  2023,  2003,  2025,  1037, 10469,  3277,  2021,  1037, 19640,\n",
      "          102,  3599,  2054,  2112,  1997,  2023,  2003,  1037, 19640,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 4952, 2000, 2023, 9866, 1012,  102, 2073, 1996, 6616, 2106, 2017,\n",
      "        2131, 2008, 2039, 8612, 1029,  102,    0,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]), 'label': tensor(0)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  1996,  2866,  2003, 21746,  1012,   102,  1028,  1996,  1066,\n",
      "         1066,  2866,  1066,  1066,  2088,  2003, 21746,  3027, 12031,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 2821, 6616, 2125, 4246, 4246, 1010, 2023, 2003, 2074,  102, 8840,\n",
      "        2140, 2017, 2064, 2467, 3962, 1037, 9398,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  1000,  2008,  1005,  1055,  2367,  1012,  1000,   102,  2017,\n",
      "         1005,  2128,  2157,  1012,  1996, 27312,  2758,  2009,  1005,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 1996, 2755, 2008, 2017, 2228, 2008, 2003, 7182, 2005,  102, 2025,\n",
      "        2108, 2583, 2000, 2424, 1037, 3105, 2005,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  2138,  2009,  1005,  1055,  2025,  2995,  2017,  8239, 16374,\n",
      "          102,  2064,  1005,  1056,  5047,  1996,  3606, 17012,  7382,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(2)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([ 101, 2054, 2055, 1996, 2033, 4168, 1000, 8840, 2140, 3287,  102, 3287,\n",
      "        4000, 2003, 7455, 2243, 1048, 2213, 7011,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "<class 'dict'>\n",
      "{'input_ids': tensor([  101,  2023,  2003,  2025,  1037, 10469,  3277,  2021,  1037, 19640,\n",
      "          102,  3599,  2054,  2112,  1997,  2023,  2003,  1037, 19640,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.396104335784912, metrics={'train_runtime': 109.0168, 'train_samples_per_second': 0.11, 'train_steps_per_second': 0.028, 'total_flos': 123336629280.0, 'train_loss': 1.396104335784912, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "        print(type(item))\n",
    "        print(item)\n",
    "        print(item['input_ids'].size())\n",
    "        print(item['token_type_ids'].size())\n",
    "        print(item['attention_mask'].size())\n",
    "        return item\n",
    "\n",
    "# Define the training and validation datasets using DataLoader\n",
    "\n",
    "train_dataset = TokenizedDataset(tokenized_train_data)\n",
    "val_dataset = TokenizedDataset(tokenized_val_data)\n",
    "\n",
    "\n",
    "\n",
    "# Define the training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           # output directory\n",
    "    num_train_epochs=3,               # total number of training epochs\n",
    "    per_device_train_batch_size=4,   # batch size per device during training\n",
    "    per_device_eval_batch_size=4,    # batch size for evaluation\n",
    "    weight_decay=0.01,                # strength of weight decay\n",
    "    logging_dir='./logs',             # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',      # evaluation strategy to adopt during training\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a928ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
